{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Information:\n",
    "\n",
    "### Team Member 1:\n",
    "* UNI:  yo2258\n",
    "* Name:Ye Ouyang\n",
    "\n",
    "### Team Member 2 [optional]:\n",
    "* UNI:  \n",
    "* Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step0 - Import Libraries, Load Data [0 points]\n",
    "\n",
    "This is the basic step where you can load the data and create train and test sets for internal validation as per your convinience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import neighbors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFECV,RFE\n",
    "\n",
    "\n",
    "\n",
    "dataPath = \"data/data.csv\"\n",
    "dataraw = pd.read_csv(dataPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1 - Exploration and Preparation [10 points]\n",
    "\n",
    "In this step, we expect you to look into the data and try to understand it before modeling. This understanding may lead to some basic data preparation steps which are common across the two model sets required.\n",
    "********************************************************************************\n",
    "1) Remove the \"Duration\" attribute which is not related with the modeling\n",
    "\n",
    "2) Convert categorical data to numeric data \n",
    "\n",
    "3) Extract traning and target\n",
    "\n",
    "4) Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write code below, y\n",
    "data = dataraw.drop('duration', 1)\n",
    "\n",
    "data = pd.get_dummies(data)\n",
    "X = data.iloc[:,:-2]\n",
    "y = data.iloc[:,-1:]    \n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=0)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "def featureSelection(model):\n",
    "    rfe = RFE(model,n_features_to_select=1)\n",
    "    rfe.fit(X_train,y_train)\n",
    "    print rfe.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step2 - ModelSet1 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set1:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. Any classification algorithm covered in class apart from tree-based models can be tested here.\n",
    "\n",
    "1) Use five models: NC/KNN/SVM/SGD/Logistic Regression\n",
    "\n",
    "2) The feature engineering/validation/feature selection have been done in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelset1():    \n",
    "    # 1: Nearest Centroid    \n",
    "    nc = NearestCentroid().fit(X_train, y_train)\n",
    "    print (nc.score(X_test,y_test))\n",
    "    \n",
    "    # 2: KNN  \n",
    "    knns = neighbors.KNeighborsClassifier(n_neighbors=3).fit(X_train,y_train)\n",
    "    print (knns.score(X_test,y_test))\n",
    "    \n",
    "    # 3: SVM      \n",
    "    linearsvm = SVC().fit(X_train,y_train)\n",
    "    print (linearsvm.score(X_test,y_test))\n",
    "    \n",
    "    # 4: SGD      \n",
    "    clf = SGDClassifier(loss=\"hinge\").fit(X_train,y_train)\n",
    "    print (clf.score(X_test,y_test))\n",
    "    \n",
    "    # 5: Logistic Regression  \n",
    "    logreg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\").fit(X_train,y_train)\n",
    "    print (logreg.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step3 - ModelSet2 [35 points]\n",
    "\n",
    "In this step, we expect you to perform the following steps relevant to the models you choose for set2:\n",
    "\n",
    "* feature engineering\n",
    "* validation\n",
    "* feature selection\n",
    "* final model selection\n",
    "\n",
    "You may select up to 5 models in this step for the purpose of final ensemble. We encourage you to try decition tree, random forest and gradient boosted tree methods here and pick the one which you think works best.\n",
    "\n",
    "\n",
    "1) Use five models: Decision Tree/Random Forest/Gradient boosted Tree\n",
    "\n",
    "2) The feature engineering/validation/feature selection have been done in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelset2():    \n",
    "    # 1: Decision Tree\n",
    "    dt = DecisionTreeClassifier().fit(X_train,y_train)\n",
    "    print (dt.score(X_test,y_test))\n",
    "    \n",
    "    # 2: Random Forest\n",
    "    rf = RandomForestClassifier().fit(X_train,y_train)\n",
    "    print(rf.score(X_test,y_test))\n",
    "    \n",
    "    # 3: Gradient boosted Tree\n",
    "    gbc = GradientBoostingClassifier().fit(X_train,y_train)\n",
    "    print(gbc.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step4 - Ensemble [20 points + 10 Bonus points]\n",
    "\n",
    "In this step, we expect you to use the models created before and create new predictions. You should definitely try poor man's stacking but we encourage you to think of different ensemble techniques as well. We will judge your creativity and improvement in model performance using ensemble models and you can potentially earn 10 bonus points here.\n",
    "\n",
    "1) From Modelset1: KNeighborsClassifier and LogisticRegression have good acurracy\n",
    "\n",
    "2) From Modelset2: Decision Tree and Random Forest have good accuracy\n",
    "\n",
    "3) Use VotingClassifier to ensemble the best results generated from the above two steps\n",
    "    - LogisticRegression\n",
    "    - KNeighborsClassifier\n",
    "    - Decision Tree\n",
    "    - Random Forest\n",
    "\n",
    "4) ROC figure is draw here to show the performance of classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensemble():\n",
    "    # Ensemble Clustering\n",
    "    voting = VotingClassifier([('logreg',LogisticRegression(C=100)),\n",
    "                               ('knn',KNeighborsClassifier()),\n",
    "                               ('Dt',DecisionTreeClassifier()),\n",
    "                               ('RF',RandomForestClassifier())]\n",
    "                              ,voting='soft')\n",
    "    model = voting.fit(X_train,y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print model.score(X_test,y_test) \n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "    plt.title('ROC for Ensemble Classification')\n",
    "    plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.2])\n",
    "    plt.ylim([-0.1,1.2])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    return voting.score(X_test,y_test)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_accuracy():\n",
    "    assert(ensemble()>0.89)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
